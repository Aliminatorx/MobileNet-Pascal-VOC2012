{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MobileNet-GCS2208",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNBOc4MRWFBsQZAdsR22++8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aliminatorx/MobileNet-Pascal-VOC2012/blob/main/MobileNet_GCS2208.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdY00H_TBWlI",
        "outputId": "f3bc609e-cdff-42fc-a800-43d322a7858a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing other libraries\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Dropout, MaxPooling2D, BatchNormalization, Activation, Flatten, Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "vxIZilXABvYh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow # cv2.imshow for COLAB\n",
        "#from cv2 import imshow as cv2_imshow # <--- use this for regular use.\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "from skimage import io\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import tarfile\n",
        "import requests\n",
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "def ensure_dir(file_path):\n",
        "    directory = os.path.dirname(file_path)\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "class VOCDataset():\n",
        "  def __init__(self):\n",
        "    self.train_dir=None\n",
        "    self.test_dir=None\n",
        "    self.trainDataLink=None\n",
        "    self.testDataLink=None\n",
        "\n",
        "    self.common_init()\n",
        "\n",
        "  def common_init(self):\n",
        "    # init that must be shared among all subclasses of this method\n",
        "    self.label_type=['none','aeroplane',\"Bicycle\",'bird',\"Boat\",\"Bottle\",\"Bus\",\"Car\",\"Cat\",\"Chair\",'cow',\"Diningtable\",\"Dog\",\"Horse\",\"Motorbike\",'person', \"Pottedplant\",'sheep',\"Sofa\",\"Train\",\"TVmonitor\"]\n",
        "    self.convert_id=['background','Aeroplane',\"Bicycle\",'Bird',\"Boat\",\"Bottle\",\"Bus\",\"Car\",\"Cat\",\"Chair\",'Cow',\"Dining table\",\"Dog\",\"Horse\",\"Motorbike\",'Person', \"Potted plant\",'Sheep',\"Sofa\",\"Train\",\"TV/monitor\"]\n",
        "    self.convert_labels={}\n",
        "    for idx, x in enumerate(self.label_type):\n",
        "      self.convert_labels[x.lower()]=idx\n",
        "\n",
        "    self.num_classes=len(self.label_type) # 20 + 1(none)\n",
        "\n",
        "  def download_dataset(self, validation_size=5000):\n",
        "    # download voc train dataset\n",
        "    print('[*] Downloading dataset...')\n",
        "    print(self.trainDataLink)\n",
        "    urllib.request.urlretrieve(self.trainDataLink, 'voctrain.tar')\n",
        "\n",
        "    print('[*] Extracting dataset...')\n",
        "    tar = tarfile.open('voctrain.tar', \"r:\")\n",
        "    tar.extractall('/content/VOCtrain')\n",
        "    tar.close()\n",
        "    os.remove('/content/voctrain.tar')\n",
        "\n",
        "    if self.testDataLink is None: \n",
        "      # move 5K images to validation set\n",
        "      print('[*] Moving validation data...')\n",
        "      ensure_dir(self.test_dir+'/Annotations/')\n",
        "      ensure_dir(self.test_dir+'/JPEGImages/')\n",
        "\n",
        "      random.seed(42)\n",
        "      val_images = random.sample(sorted(os.listdir(self.train_dir + '/JPEGImages')), validation_size)\n",
        "\n",
        "      for path in val_images:\n",
        "        img_name = path.split('/')[-1].split('.')[0]\n",
        "        # move image\n",
        "        os.rename(self.train_dir+'/JPEGImages/'+img_name+'.jpg', self.test_dir+'/JPEGImages/'+img_name+'.jpg')\n",
        "        # move annotation\n",
        "        os.rename(self.train_dir+'/Annotations/'+img_name+'.xml', self.test_dir+'/Annotations/'+img_name+'.xml')\n",
        "    else: \n",
        "      # Load from val data\n",
        "      print('[*] Downloading validation dataset...')\n",
        "      urllib.request.urlretrieve(self.testDataLink, 'voctest.tar')\n",
        "\n",
        "      print('[*] Extracting validation dataset...')\n",
        "      tar = tarfile.open('voctest.tar', \"r:\")\n",
        "      tar.extractall('/content/VOCtest')\n",
        "      tar.close()\n",
        "      os.remove('/content/voctest.tar')\n",
        "\n",
        "  def read_xml(self, xml_path): \n",
        "    object_list=[]\n",
        "\n",
        "    tree = ET.parse(open(xml_path, 'r'))\n",
        "    root=tree.getroot()\n",
        "  \n",
        "    objects = root.findall(\"object\")\n",
        "    for _object in objects:\n",
        "      name = _object.find(\"name\").text\n",
        "      bndbox = _object.find(\"bndbox\")\n",
        "      xmin = int(bndbox.find(\"xmin\").text)\n",
        "      ymin = int(bndbox.find(\"ymin\").text)\n",
        "      xmax = int(bndbox.find(\"xmax\").text)\n",
        "      ymax = int(bndbox.find(\"ymax\").text)\n",
        "      class_name = _object.find('name').text\n",
        "      object_list.append({'x1':xmin, 'x2':xmax, 'y1':ymin, 'y2':ymax, 'class': self.convert_labels[class_name]})\n",
        "\n",
        "    return object_list\n",
        "class VOC2007(VOCDataset):\n",
        "  def __init__(self):\n",
        "    self.train_dir='/content/VOCtrain/VOCdevkit/VOC2007'\n",
        "    self.test_dir='/content/VOCtest/VOCdevkit/VOC2007'\n",
        "    self.trainDataLink='http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar'\n",
        "    self.testDataLink='http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar'\n",
        "    self.common_init()#mandatory\n",
        "    \n",
        "class VOC2012(VOCDataset):\n",
        "  def __init__(self):\n",
        "    self.train_dir='/content/VOCtrain/VOCdevkit/VOC2012'\n",
        "    self.test_dir='/content/VOCtest/VOCdevkit/VOC2012'\n",
        "    # original site goes down frequently, so we use a link to the clone alternatively\n",
        "    # self.trainDataLink='http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar' \n",
        "    self.trainDataLink = 'http://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar'\n",
        "    self.testDataLink=None\n",
        "    self.common_init()#mandatory"
      ],
      "metadata": {
        "id": "al9CepU8BzDO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voc_dataset=VOC2012()\n",
        "voc_dataset.download_dataset()\n",
        "val_datalen=len(os.listdir(voc_dataset.test_dir+'/Annotations'))\n",
        "train_datalen=len(os.listdir(voc_dataset.train_dir+'/Annotations'))\n",
        "print(train_datalen, val_datalen) # 17,125 total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1NEuy9HBzO8",
        "outputId": "3d868782-0cb9-4d9b-ecc7-96f2e0415a96"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Downloading dataset...\n",
            "http://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar\n",
            "[*] Extracting dataset...\n",
            "[*] Moving validation data...\n",
            "12125 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trdata = ImageDataGenerator()\n",
        "traindata = trdata.flow_from_directory(directory=\"/content/VOCtrain/VOCdevkit/VOC2012\",target_size=(227,227))\n",
        "tsdata = ImageDataGenerator()\n",
        "testdata = tsdata.flow_from_directory(directory=\"/content/VOCtest/VOCdevkit/VOC2012\", target_size=(227,227))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph4HgBocB6AR",
        "outputId": "51fe4ed5-797f-4385-a58f-cd534ac0b4b0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 17951 images belonging to 5 classes.\n",
            "Found 5000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SETS_PATH = '/content/VOCtrain/VOCdevkit/VOC2012/ImageSets/'\n",
        "IMAGE_SETS_MAIN_PATH = '/content/VOCtrain/VOCdevkit/VOC2012/ImageSets/Main/'\n",
        "JPEG_IMAGES_PATH = '/content/VOCtrain/VOCdevkit/VOC2012/JPEGImages/'"
      ],
      "metadata": {
        "id": "mbAyv2G1CJYa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imageSets_list = os.listdir(IMAGE_SETS_PATH + 'Main')"
      ],
      "metadata": {
        "id": "WLbNLCpwCMFH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file_list = []\n",
        "val_file_list = []\n",
        "\n",
        "for file_name in imageSets_list:\n",
        "    if file_name.find('_train.txt') != -1:\n",
        "        train_file_list.append(file_name)\n",
        "    elif file_name.find('_val.txt') != -1:\n",
        "        val_file_list.append(file_name)"
      ],
      "metadata": {
        "id": "5kfrZpqFCRuY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "AuH71h2rCUAF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataframe = pd.DataFrame(columns=('file_name', 'class'))\n",
        "val_dataframe = pd.DataFrame(columns=('file_name', 'class'))\n",
        "\n",
        "kind = 0\n",
        "for train_file_name in train_file_list:\n",
        "    f = open(IMAGE_SETS_MAIN_PATH + train_file_name)\n",
        "    row_list = f.readlines()\n",
        "    for pair in row_list:\n",
        "        target = pair.split()\n",
        "        if target[1] == '1':\n",
        "            train_dataframe = train_dataframe.append(\n",
        "                [{'file_name': JPEG_IMAGES_PATH + target[0] + '.jpg',\n",
        "                  'class': train_file_name[:-10]}],\n",
        "                ignore_index=True\n",
        "            )\n",
        "    kind += 1\n",
        "    f.close()\n",
        "\n",
        "kind = 0\n",
        "for val_file_name in val_file_list:\n",
        "    f = open(IMAGE_SETS_MAIN_PATH + val_file_name)\n",
        "    row_list = f.readlines()\n",
        "    for pair in row_list:\n",
        "        target = pair.split()\n",
        "        if target[1] == '1':\n",
        "            val_dataframe = val_dataframe.append(\n",
        "                [{'file_name': JPEG_IMAGES_PATH + target[0] + '.jpg',\n",
        "                  'class': val_file_name[:-8]}],\n",
        "                ignore_index=True\n",
        "            )\n",
        "    kind += 1\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "Rc89Ft0mCZZO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=keras.applications.mobilenet.preprocess_input,\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "test_generator = keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=keras.applications.mobilenet.preprocess_input\n",
        ")"
      ],
      "metadata": {
        "id": "hflRq0HRCg7a"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sets = train_generator.flow_from_dataframe(\n",
        "    dataframe=train_dataframe,\n",
        "    x_col='file_name',\n",
        "    y_col='class',\n",
        "    target_size=(300, 300),\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    subset='training',\n",
        "    rotation_range=30,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "val_sets = train_generator.flow_from_dataframe(\n",
        "    dataframe=train_dataframe,\n",
        "    x_col='file_name',\n",
        "    y_col='class',\n",
        "    target_size=(300, 300),\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    seed=0,\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "test_sets = test_generator.flow_from_dataframe(\n",
        "    dataframe=val_dataframe,\n",
        "    x_col='file_name',\n",
        "    y_col='class',\n",
        "    target_size=(300, 300),\n",
        "    color_mode='rgb',\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLdC4ttNCkPR",
        "outputId": "c3a6581c-915e-44a8-b7f0-4ed637281f95"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5261 validated image filenames belonging to 20 classes.\n",
            "Found 584 validated image filenames belonging to 20 classes.\n",
            "Found 5964 validated image filenames belonging to 20 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/dataframe_iterator.py:282: UserWarning: Found 2486 invalid image filename(s) in x_col=\"file_name\". These filename(s) will be ignored.\n",
            "  .format(n_invalid, x_col)\n",
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/dataframe_iterator.py:282: UserWarning: Found 2387 invalid image filename(s) in x_col=\"file_name\". These filename(s) will be ignored.\n",
            "  .format(n_invalid, x_col)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Build MobileNet Model\n",
        "original_model = keras.applications.mobilenet.MobileNet(\n",
        "    input_shape=(300, 300, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    pooling='avg'\n",
        ")\n",
        "\n",
        "inputs = original_model.input\n",
        "\n",
        "x = keras.layers.Dense(512, activation='relu')(original_model.output)\n",
        "x = keras.layers.Dense(256, activation='relu')(x)\n",
        "\n",
        "outputs = keras.layers.Dense(20, activation='softmax')(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(loss='mse', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "lrqAfpTcCprA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a75de759-218e-4b02-f7bc-c3ccc8e42d9c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
            "17227776/17225924 [==============================] - 0s 0us/step\n",
            "17235968/17225924 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"mobilenet_model.h5\",\n",
        "                                                        monitor='val_loss',\n",
        "                                                        verbose=1,\n",
        "                                                        save_best_only=True,\n",
        "                                                        mode='min')\n",
        "\n",
        "history = model.fit(train_sets, \n",
        "                    epochs =20, \n",
        "                    verbose=1,\n",
        "                    validation_data=val_sets,\n",
        "                    callbacks=[checkpoint])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9QMmujO0-2l",
        "outputId": "5cf1f953-99db-4755-9f41-29602ef16774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "165/165 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.4085 \n",
            "Epoch 1: val_loss improved from inf to 0.06593, saving model to densenet_model.h5\n",
            "165/165 [==============================] - 1786s 11s/step - loss: 0.0387 - accuracy: 0.4085 - val_loss: 0.0659 - val_accuracy: 0.0993\n",
            "Epoch 2/20\n",
            "165/165 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.4874 \n",
            "Epoch 2: val_loss did not improve from 0.06593\n",
            "165/165 [==============================] - 1760s 11s/step - loss: 0.0345 - accuracy: 0.4874 - val_loss: 0.0717 - val_accuracy: 0.0103\n",
            "Epoch 3/20\n",
            "165/165 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.5280 \n",
            "Epoch 3: val_loss improved from 0.06593 to 0.06508, saving model to densenet_model.h5\n",
            "165/165 [==============================] - 1782s 11s/step - loss: 0.0320 - accuracy: 0.5280 - val_loss: 0.0651 - val_accuracy: 0.0685\n",
            "Epoch 4/20\n",
            "165/165 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.5374 \n",
            "Epoch 4: val_loss did not improve from 0.06508\n",
            "165/165 [==============================] - 1773s 11s/step - loss: 0.0307 - accuracy: 0.5374 - val_loss: 0.0659 - val_accuracy: 0.0308\n",
            "Epoch 5/20\n",
            "165/165 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.5564 \n",
            "Epoch 5: val_loss improved from 0.06508 to 0.06250, saving model to densenet_model.h5\n",
            "165/165 [==============================] - 1768s 11s/step - loss: 0.0296 - accuracy: 0.5564 - val_loss: 0.0625 - val_accuracy: 0.0599\n",
            "Epoch 6/20\n",
            "165/165 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.5858 \n",
            "Epoch 6: val_loss improved from 0.06250 to 0.06129, saving model to densenet_model.h5\n",
            "165/165 [==============================] - 1778s 11s/step - loss: 0.0281 - accuracy: 0.5858 - val_loss: 0.0613 - val_accuracy: 0.0514\n",
            "Epoch 7/20\n",
            "165/165 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.5892 \n",
            "Epoch 7: val_loss improved from 0.06129 to 0.05751, saving model to densenet_model.h5\n",
            "165/165 [==============================] - 1739s 11s/step - loss: 0.0276 - accuracy: 0.5892 - val_loss: 0.0575 - val_accuracy: 0.1199\n",
            "Epoch 8/20\n",
            "165/165 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.6025 \n",
            "Epoch 8: val_loss improved from 0.05751 to 0.05619, saving model to densenet_model.h5\n",
            "165/165 [==============================] - 1765s 11s/step - loss: 0.0268 - accuracy: 0.6025 - val_loss: 0.0562 - val_accuracy: 0.1353\n",
            "Epoch 9/20\n",
            "165/165 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.6168 \n",
            "Epoch 9: val_loss did not improve from 0.05619\n",
            "165/165 [==============================] - 1755s 11s/step - loss: 0.0260 - accuracy: 0.6168 - val_loss: 0.0626 - val_accuracy: 0.0531\n",
            "Epoch 10/20\n",
            "165/165 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.6271 \n",
            "Epoch 10: val_loss did not improve from 0.05619\n",
            "165/165 [==============================] - 1754s 11s/step - loss: 0.0248 - accuracy: 0.6271 - val_loss: 0.0700 - val_accuracy: 0.0565\n",
            "Epoch 11/20\n",
            "165/165 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.6240 \n",
            "Epoch 11: val_loss did not improve from 0.05619\n",
            "165/165 [==============================] - 1760s 11s/step - loss: 0.0250 - accuracy: 0.6240 - val_loss: 0.0652 - val_accuracy: 0.0445\n",
            "Epoch 12/20\n",
            "165/165 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.6341 \n",
            "Epoch 12: val_loss did not improve from 0.05619\n",
            "165/165 [==============================] - 1919s 12s/step - loss: 0.0241 - accuracy: 0.6341 - val_loss: 0.0648 - val_accuracy: 0.0514\n",
            "Epoch 13/20\n",
            "165/165 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.6345 \n",
            "Epoch 13: val_loss did not improve from 0.05619\n",
            "165/165 [==============================] - 1743s 11s/step - loss: 0.0242 - accuracy: 0.6345 - val_loss: 0.0721 - val_accuracy: 0.0308\n",
            "Epoch 14/20\n",
            "165/165 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.6362 \n",
            "Epoch 14: val_loss did not improve from 0.05619\n",
            "165/165 [==============================] - 1785s 11s/step - loss: 0.0241 - accuracy: 0.6362 - val_loss: 0.0676 - val_accuracy: 0.0616\n",
            "Epoch 15/20\n",
            "165/165 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.6600 \n",
            "Epoch 15: val_loss did not improve from 0.05619\n",
            "165/165 [==============================] - 1808s 11s/step - loss: 0.0224 - accuracy: 0.6600 - val_loss: 0.0717 - val_accuracy: 0.0497\n",
            "Epoch 16/20\n",
            "165/165 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.6447 \n",
            "Epoch 16: val_loss did not improve from 0.05619\n",
            "165/165 [==============================] - 1805s 11s/step - loss: 0.0234 - accuracy: 0.6447 - val_loss: 0.0700 - val_accuracy: 0.0634\n",
            "Epoch 17/20\n",
            "165/165 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.6638 \n",
            "Epoch 17: val_loss did not improve from 0.05619\n",
            "165/165 [==============================] - 1805s 11s/step - loss: 0.0226 - accuracy: 0.6638 - val_loss: 0.0637 - val_accuracy: 0.0856\n",
            "Epoch 18/20\n",
            "165/165 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.6685 \n",
            "Epoch 18: val_loss did not improve from 0.05619\n",
            "165/165 [==============================] - 1810s 11s/step - loss: 0.0216 - accuracy: 0.6685 - val_loss: 0.0698 - val_accuracy: 0.0445\n",
            "Epoch 19/20\n",
            "165/165 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.6638 \n",
            "Epoch 19: val_loss did not improve from 0.05619\n",
            "165/165 [==============================] - 1773s 11s/step - loss: 0.0217 - accuracy: 0.6638 - val_loss: 0.0721 - val_accuracy: 0.0942\n",
            "Epoch 20/20\n",
            "165/165 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.6639 \n",
            "Epoch 20: val_loss did not improve from 0.05619\n",
            "165/165 [==============================] - 1747s 11s/step - loss: 0.0217 - accuracy: 0.6639 - val_loss: 0.0735 - val_accuracy: 0.0616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_sets, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4YXb2gC97Wj",
        "outputId": "64a0b75e-16d2-4106-c977-a03ab88cdb66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "187/187 [==============================] - 357s 2s/step - loss: 0.0366 - accuracy: 0.4930\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.03656448423862457, 0.49295774102211]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('VOC_MobileNet.h5')"
      ],
      "metadata": {
        "id": "UiqKv2ME-YuR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}